# Interface config.
interface:
  framework: http+grpc # `http`, `http+grpc`.
  host: 0.0.0.0
  port: 7080,7081 # http port, grpc port.
  #customized_predict_http: # customized predict http config. If you want to enable it, please uncomment this section.
  #  path: /custom_predict # customized predict http path.
  #  customized_body: true # whether to use customized predict http body. If true, user should parse request and build response themselves.
  #  streaming_ctrl: # user can control if streaming and response content type.
  #    ctrl_mode: # `query_param`, `header_param` or `body_param`(only json body is supported). If not set, will use `query_param`.
  #    ctrl_key: # key of control parameter. If not set, will use `streaming`. (`ctrl_key`=true) means streaming.
  #    res_content_type: # response content type. If not set, will use `application/octet-stream`.

max_connections: 1000 # Maximum number of concurrent connections.
max_concurrency: 32 # Maximum parallel request limit; requests exceeding it will be queued.

# Gpu monitor config(Optional). Include gpu utilization and gpu memory monitor. If you don't need it, just comment it.
gpu:
  devices: [0] # Devices will be monitored.
  mem_manager_type: none # `torch`, `tensorflow` or `none`. If not none, uncomment following configs to enable gpu memory manager.
  #mem_limit_mib: 4096 # gpu memory limit with MiB unit, -1 means no limit.
  #mem_gc_enable: false # If enable gpu memory gc.
  #mem_gc_interval: 60 # Interval(s) of memery garbage collection.

# Log config.
log:
  log_dir: ./logs # Log dir. Will be subdir of deploy path if is relative path.
  log_backup_count: 7 # Number of log files to keep. One log file per day.
